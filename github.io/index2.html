<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>CaltechCameraTraps | Caltech Camera Trap dataset page</title>
<meta name="generator" content="Jekyll v3.8.5">
<meta property="og:title" content="CaltechCameraTraps">
<meta property="og:locale" content="en_US">
<meta name="description" content="Caltech Camera Trap dataset page">
<meta property="og:description" content="Caltech Camera Trap dataset page">
<link rel="canonical" href="">
<meta property="og:url" content="https://beerys.github.io/CaltechCameraTraps/">
<meta property="og:site_name" content="CaltechCameraTraps">
<script type="application/ld+json">
{"@type":"WebSite","url":"https://beerys.github.io/CaltechCameraTraps/","headline":"CaltechCameraTraps","name":"CaltechCameraTraps","description":"Caltech Camera Trap dataset page","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="static/css/style1.css">
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body">
      
      <h1><a href="">CaltechCameraTraps</a></h1>
      

      <link rel="apple-touch-icon" sizes="180x180" href="https://rawgit.com/visipedia/iwildcam_comp/master/assets/apple-touch-icon.png">

<link rel="icon" type="image/png" sizes="32x32" href="https://rawgit.com/visipedia/iwildcam_comp/master/assets/favicon-32x32.png">

<link rel="icon" type="image/png" sizes="16x16" href="https://rawgit.com/visipedia/iwildcam_comp/master/assets/favicon-16x16.png">

<link rel="manifest" href="https://rawgit.com/visipedia/iwildcam_comp/master/assets/site.webmanifest">

<link rel="mask-icon" href="https://rawgit.com/visipedia/iwildcam_comp/master/assets/safari-pinned-tab.svg" color="#5bbad5">

<link rel="shortcut icon" href="https://rawgit.com/visipedia/iwildcam_comp/master/assets/favicon.ico">

<meta name="msapplication-TileColor" content="#da532c">

<meta name="msapplication-config" content="https://rawgit.com/visipedia/iwildcam_comp/master/assets/browserconfig.xml">

<meta name="theme-color" content="#ffffff">

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/CaltechCameraTraps2.jpg" alt="Banner"></p>

<h1 id="caltech-camera-traps-cct">Caltech Camera Traps (CCT)</h1>
<p>Camera traps are motion- or heat-triggered cameras that are placed in locations of interest by biologists in order to monitor and study animal populations and behavior. When a camera is triggered, a sequence of images is taken at approximately one frame per second. The cameras are prone to false triggers caused by wind or heat rising from the ground, leading to empty frames. Empty frames can also occur if an animal moves out of the field of view of the camera while the sequence is firing. Once a month, biologists return to the cameras to replace the batteries and change out the memory card. After it has been collected, experts manually sort camera trap data to categorize species and remove empty frames. These cameras enable the automatic collection of large quantities of image data, but the time required to sort images severely limits data scale and research productivity. Automating the process using computer vision would make camera trap research scalable and efficient.</p>

<p>The goal of this dataset is to provide a testbed for computer vision researchers to investigate how well their models are able to generalize to unseen locations. The main challenge is generalizing to a diverse set of unseen camera trap locations that have been captured both during the day and at night that are not present in the training set. The dataset can be used to predict if images  contain an animal, to detect that animal, and to classify the animal. Some images contain other objects (e.g. people or vehicles) that can trigger the cameras but are not of interest. The animals of interest can be very small, partially occluded, or exiting the frame - you sometimes have to look hard to find them. There may also be a small number of incorrect annotations in the training set.</p>

<p>This dataset is a superset of the data used in our ECCV18 paper <a href="javascript:;">“Recognition in Terra Incognita”</a> and was used in the <a href="javascript:;">iWildCam 2018 FGVCx competition</a> as part of the <a href="javascript:;">FGVC^5 workshop</a> at CVPR18 and the <a href="javascript:;">iWildCam 2019 FGVCx competition</a> as part of the <a href="javascript:;">FGVC^6 workshop</a> at CVPR19. Please email sbeery at caltech dot edu if you have questions or problems with the dataset.</p>

<h2 id="details-and-evaluation">Details and Evaluation</h2>

<p>The dataset contains 243,187 images from 140 camera locations.<br>
For the iWildCam Challenge 2018 we provided a split containing 106,428 training images from 65 different camera locations and 12,719 validation images from 10 new locations not seen at training time. The test set contained 124,040 images from 65 locations that are not present in the training or validation sets. The location id (<code class="highlighter-rouge">location</code>) is given for all images.</p>

<p>The evaluation metric for the iWildCam18 challenge was overall accuracy in a binary animal/no animal classification task i.e. correctly predicting which of the test images contain animals.</p>

<p>This dataset has class-level annotations for all images, as well as bounding box annotations for a subset of 57,864 images from 20 locations.  The subset containing bounding boxes was used for our ECCV18 paper, “Recognition in Terra Incognita.”</p>

<h2 id="annotation-format">Annotation Format</h2>
<p>We follow the annotation format of the <a href="javascript:;">COCO dataset</a> and add additional fields in order to specify camera-trap specific information.  These fields include a location id, a sequence id, the number of frames in that sequence, and the frame number of the individual image.  Note that not all cameras take sequences of images at a single trigger, so for some images the number of frames in the associated sequence will be one.Each training image has a <code class="highlighter-rouge">category_id</code> representing its class, and some images also have a <code class="highlighter-rouge">bbox</code>. The annotations are stored in the <a href="javascript:;">JSON format</a> and are organized as follows:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "info" : info,
  "images" : [image],
  "categories" : [category],
  "annotations" : [annotation]
}

info{
  "year" : int,
  "version" : str,
  "description" : str,
  "contributor" : str
  "date_created" : datetime
}

image{
  "id" : str,
  "width" : int,
  "height" : int,
  "file_name" : str,
  "rights_holder" : str,
  "location": int,
  "datetime": datetime,
  "seq_id": str,
  "seq_num_frames": int,
  "frame_num": int
}

category{
  "id" : int,
  "name" : str
}

annotation{
  "id" : str,
  "image_id" : str,
  "category_id" : int,
  # These are in absolute, floating-point coordinates, with the origin at the upper-left
  "bbox": [x,y,width,height]
}
</code></pre></div></div>

<h2 id="terms-of-use">Terms of Use</h2>

<p>By downloading this dataset you agree to the terms outlined in the <a href="javascript:;">Community Data License Agreement (CDLA)</a>.</p>

<h2 id="data">Data</h2>

<p>The data can be downloaded from the <a href="javascript:;">Labeled Information Library of Alexandria</a></p>

<p>Images, splits, and annotations from the Caltech Camera Traps-20 (CCT-20) data subset used in our ECCV18 paper can be seperately downloaded:</p>
<ul>
  <li>All ECCV18 images 22 GB (Contains 1.3K images not included in the iWildCam 2018 Challenge Dataset)
    <ul>
      <li><a href="javascript:;">Link</a></li>
      <li>Running <code class="highlighter-rouge">md5sum eccv_18_all_images.tar.gz</code> should produce <code class="highlighter-rouge">04c0f27699d0643fcabfd28992d1189f</code></li>
    </ul>
  </li>
  <li>All ECCV18 annotations 2975KB
    <ul>
      <li><a href="javascript:;">Link</a></li>
      <li>Running <code class="highlighter-rouge">md5sum eccv_18_all_annotations.tar.gz</code> should produce <code class="highlighter-rouge">90bf977d435c72139404c13b7ab39982</code></li>
    </ul>
  </li>
  <li>Separate json files for ECCV18 data split 2927KB
    <ul>
      <li><a href="javascript:;">Link</a></li>
      <li>Running <code class="highlighter-rouge">md5sum eccv_18_annotations.tar.gz</code> should produce <code class="highlighter-rouge">66a1f481b44aa1edadf75c9cfbd27aba</code></li>
    </ul>
  </li>
  <li>Smaller ECCV18 images, image width resized to 1024 pixels
    <ul>
      <li><a href="javascript:;">Link</a></li>
      <li>Running <code class="highlighter-rouge">md5sum eccv_18_all_images_sm.tar.gz</code> should produce <code class="highlighter-rouge">8143c17aa2a12872b66f284ff211531f</code></li>
    </ul>
  </li>
</ul>

<h2 id="notes-on-the-eccv-paper-recognition-in-terra-incognita">Notes on the ECCV Paper “Recognition in Terra Incognita”</h2>

<h3 id="abstract">Abstract</h3>
<p>It is desirable for detection and classification algorithms to
generalize to unfamiliar environments, but suitable benchmarks for quantitatively
studying this phenomenon are not yet available. We present a
dataset designed to measure recognition generalization to novel environments.
The images in our dataset are harvested from twenty camera traps
deployed to monitor animal populations. Camera traps are fixed at one
location, hence the background changes little across images; capture is
triggered automatically, hence there is no human bias. The challenge is
learning recognition in a handful of locations, and generalizing animal
detection and classification to new locations where no training data is
available. In our experiments state-of-the-art algorithms show excellent
performance when tested at the same location where they were trained.
However, we find that generalization to new locations is poor, especially
for classification systems.</p>

<h3 id="clarifications">Clarifications</h3>

<p>We ran three types of experiments in the paper, separating animal classification from animal detection in order to analyze their contributions to the generalization gap when testing on novel environments.</p>

<ol>
  <li>Full image classification using Inception V3 for 15 animal classes, pretrained on ImageNet.</li>
  <li>Cropped ground-truth box animal classification using Inception V3 for 15 animal classes, pretrained on ImageNet.</li>
  <li>Faster-RCNN detection on full images for a single combined class “animal”, using both Resnet 101 and Inception-Resnet-V2 with atrous backbones, pretrained on COCO.</li>
</ol>

<h2 id="data-challenges">Data Challenges</h2>
<p>Camera trap data provides several challenges that can make it difficult to achieve accurate results.</p>

<h4 id="class-distributions">Class distributions</h4>
<p>Each location has its own unique distribution of classes, and overcoming these priors can be difficult.  The distribution of classes at each location is visualized in the plot below.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/LogNumImsPerLoc_FGVC_all.jpg" width="400"></p>

<h4 id="illumination">Illumination:</h4>
<p>Images can be poorly illuminated, especially at night.  The example below contains a skunk to the center left of the frame.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/illumination.png" width="400"></p>

<h4 id="motion-blur">Motion Blur:</h4>
<p>The shutter speed of the camera is not fast enough to eliminate motion blur, so animals are sometimes blurry. The example contains a blurred coyote.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/blur.png" width="400"></p>

<h4 id="small-roi">Small ROI:</h4>
<p>Some animals are small or far from the camera, and can be difficult to spot even for humans.  The example image has a mouse on a branch to the center right of the frame.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/smallroi.png" width="400"></p>

<h4 id="occlusion">Occlusion:</h4>
<p>Animals can be occluded by vegetation or the edge of the frame.  This example shows a location where weeds grew in front of the camera, obscuring the view.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/occlusion.png" width="400"></p>

<h4 id="perspective">Perspective:</h4>
<p>Sometimes animals come very close to the camera, causing a forced perspective.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/perspective.png" width="400"></p>

<h4 id="weather-conditions">Weather Conditions:</h4>
<p>Poor weather, including rain or dust, can obstruct the lens and cause false triggers.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/weather.png" width="400"></p>

<h4 id="camera-malfunctions">Camera Malfunctions:</h4>
<p>Sometimes the camera malfunctions, causing strange discolorations.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/malfunctions.png" width="400"></p>

<h4 id="temporal-changes">Temporal Changes:</h4>

<p>At any given location, the background changes over time as the seasons change.  Below, you can see a single loction at three different points in time.</p>

<p><img src="https://rawgit.com/visipedia/iwildcam_comp/master/assets/changesovertime.png" alt="alt text"></p>

<h4 id="non-animal-variability">Non-Animal Variability:</h4>
<p>What causes the non-animal images to trigger varies based on location.  Some locations contain lots of vegetation, which can cause false triggers as it moves in the wind.  Others are near roadways, so can be triggered by cars or bikers.</p>

<h3 id="acknowledgements">Acknowledgements</h3>
<p>We would like to thank Erin Boydston (USGS) and Justin Brown (NPS) for providing us with data. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. 1745301. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.</p>


      
    </div>
    <script src="static/js/anchor.min.js" integrity="sha256-lZaRhKri35AyJSypXXs4o6OPFTbTmUoltBbDCbdzegg=" crossorigin="anonymous"></script>
    <script>anchors.add();</script>
    
  </body>
</html>
